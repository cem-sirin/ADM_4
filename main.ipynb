{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Algorithmic Methods of Data Mining - Fall 2022**\n",
    "\n",
    "## **Homework 4: Getting to know your customers**\n",
    "\n",
    "**Packages that are used troughout the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "# For progress_apply\n",
    "tqdm.pandas()\n",
    "\n",
    "datas = datasets.Datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finding Similar Costumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set up the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off, below is the head of the processed transaction dataset. Briefly, we have:\n",
    "- renamed the columns to make it easier to work with,\n",
    "- created customer IDs (cid) from scratch, as the original dataset was not consistent with the customer IDs. The new customer IDs are unique identifiers for the customer DOB, location and balance columns. We have made a post about this in the Slack channel,\n",
    "- merged the transaction date and time into one column, and converted the date to a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>cid</th>\n",
       "      <th>cdob</th>\n",
       "      <th>cgender</th>\n",
       "      <th>clocation</th>\n",
       "      <th>cbalance</th>\n",
       "      <th>tdate</th>\n",
       "      <th>tamount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>0</td>\n",
       "      <td>1994-01-10</td>\n",
       "      <td>F</td>\n",
       "      <td>JAMSHEDPUR</td>\n",
       "      <td>17819.05</td>\n",
       "      <td>2016-08-02 14:32:07</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>1</td>\n",
       "      <td>2057-04-04</td>\n",
       "      <td>M</td>\n",
       "      <td>JHAJJAR</td>\n",
       "      <td>2270.69</td>\n",
       "      <td>2016-08-02 14:18:58</td>\n",
       "      <td>27999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>2</td>\n",
       "      <td>1996-11-26</td>\n",
       "      <td>F</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>17874.44</td>\n",
       "      <td>2016-08-02 14:27:12</td>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>3</td>\n",
       "      <td>1973-09-14</td>\n",
       "      <td>F</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>866503.21</td>\n",
       "      <td>2016-08-02 14:27:14</td>\n",
       "      <td>2060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>4</td>\n",
       "      <td>1988-03-24</td>\n",
       "      <td>F</td>\n",
       "      <td>NAVI MUMBAI</td>\n",
       "      <td>6714.43</td>\n",
       "      <td>2016-08-02 18:11:56</td>\n",
       "      <td>1762.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tid  cid       cdob cgender    clocation   cbalance               tdate  \\\n",
       "0  T1    0 1994-01-10       F   JAMSHEDPUR   17819.05 2016-08-02 14:32:07   \n",
       "1  T2    1 2057-04-04       M      JHAJJAR    2270.69 2016-08-02 14:18:58   \n",
       "2  T3    2 1996-11-26       F       MUMBAI   17874.44 2016-08-02 14:27:12   \n",
       "3  T4    3 1973-09-14       F       MUMBAI  866503.21 2016-08-02 14:27:14   \n",
       "4  T5    4 1988-03-24       F  NAVI MUMBAI    6714.43 2016-08-02 18:11:56   \n",
       "\n",
       "   tamount  \n",
       "0     25.0  \n",
       "1  27999.0  \n",
       "2    459.0  \n",
       "3   2060.0  \n",
       "4   1762.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = datas.getTransactions().copy()\n",
    "transactions.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the head of the customers dataset that was obtained by grouping the transactions dataset by the customer IDs. Since we believe that the DOB, gender, location, and balance are unique for each customer, we did not have to do any aggregation when grouping the transactions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>balance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994-01-10</td>\n",
       "      <td>F</td>\n",
       "      <td>JAMSHEDPUR</td>\n",
       "      <td>17819.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1957-04-04</td>\n",
       "      <td>M</td>\n",
       "      <td>JHAJJAR</td>\n",
       "      <td>2270.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-11-26</td>\n",
       "      <td>F</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>17874.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1973-09-14</td>\n",
       "      <td>F</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>866503.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-03-24</td>\n",
       "      <td>F</td>\n",
       "      <td>NAVI MUMBAI</td>\n",
       "      <td>6714.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dob gender     location    balance\n",
       "cid                                          \n",
       "0   1994-01-10      F   JAMSHEDPUR   17819.05\n",
       "1   1957-04-04      M      JHAJJAR    2270.69\n",
       "2   1996-11-26      F       MUMBAI   17874.44\n",
       "3   1973-09-14      F       MUMBAI  866503.21\n",
       "4   1988-03-24      F  NAVI MUMBAI    6714.43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers = datas.getCustomers().copy()\n",
    "customers.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create shingles for each customer by using their age, average transaction amount, and balance. To do this, we are going to divide these variables into 10% quantiles, and assign a quantile to each customer. Therefore, our shingle matrix will consist of $3\\times10=30$ rows, and a column for each customer. The values in the matrix will be 1 if the customer belongs to the corresponding quantile, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>balance</th>\n",
       "      <th>age</th>\n",
       "      <th>ave_tamount</th>\n",
       "      <th>age_group</th>\n",
       "      <th>balance_group</th>\n",
       "      <th>tamount_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994-01-10</td>\n",
       "      <td>F</td>\n",
       "      <td>JAMSHEDPUR</td>\n",
       "      <td>17819.05</td>\n",
       "      <td>22</td>\n",
       "      <td>80.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1957-04-04</td>\n",
       "      <td>M</td>\n",
       "      <td>JHAJJAR</td>\n",
       "      <td>2270.69</td>\n",
       "      <td>59</td>\n",
       "      <td>9503.445000</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-11-26</td>\n",
       "      <td>F</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>17874.44</td>\n",
       "      <td>19</td>\n",
       "      <td>940.348182</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1973-09-14</td>\n",
       "      <td>F</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>866503.21</td>\n",
       "      <td>43</td>\n",
       "      <td>2537.346923</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-03-24</td>\n",
       "      <td>F</td>\n",
       "      <td>NAVI MUMBAI</td>\n",
       "      <td>6714.43</td>\n",
       "      <td>28</td>\n",
       "      <td>1606.135000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dob gender     location    balance  age  ave_tamount  age_group  \\\n",
       "cid                                                                          \n",
       "0   1994-01-10      F   JAMSHEDPUR   17819.05   22    80.111111          0   \n",
       "1   1957-04-04      M      JHAJJAR    2270.69   59  9503.445000          9   \n",
       "2   1996-11-26      F       MUMBAI   17874.44   19   940.348182          0   \n",
       "3   1973-09-14      F       MUMBAI  866503.21   43  2537.346923          8   \n",
       "4   1988-03-24      F  NAVI MUMBAI    6714.43   28  1606.135000          3   \n",
       "\n",
       "     balance_group  tamount_group  \n",
       "cid                                \n",
       "0                5              0  \n",
       "1                2              9  \n",
       "2                5              5  \n",
       "3                9              8  \n",
       "4                3              7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the new features\n",
    "customers['age'] = ((transactions['tdate'].max() - customers['dob']).dt.days // 365).astype(int)\n",
    "customers['ave_tamount'] = transactions.groupby('cid')['tamount'].mean()\n",
    "\n",
    "# Creating quantile groups\n",
    "quantiles = 10\n",
    "customers['age_group'] = pd.qcut(customers['age'], quantiles, labels=False)\n",
    "customers['balance_group'] = pd.qcut(customers['balance'], quantiles, labels=False)\n",
    "customers['tamount_group'] = pd.qcut(customers['ave_tamount'], quantiles, labels=False)\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to store the intervals of the quantiles, since we will use them again when querying for similar customers. Futhermore we will need a function that give us the quantile group of a value, given the intervals of the quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the quantile intervals\n",
    "age_groups = pd.qcut(customers['age'], quantiles).unique().sort_values()\n",
    "balance_groups = pd.qcut(customers['balance'], quantiles).unique().sort_values()\n",
    "tamount_groups = pd.qcut(customers['ave_tamount'], quantiles).unique().sort_values()\n",
    "\n",
    "# Get the group of a value\n",
    "def getGroup(group, value):\n",
    "    return np.where([value in g for g in group])[0][0] if len(np.where([value in g for g in group])[0]) > 0 else None\n",
    "\n",
    "getGroup(age_groups, 29) # Test: should return 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create our shingle matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cid</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>162301</th>\n",
       "      <th>162302</th>\n",
       "      <th>162303</th>\n",
       "      <th>162304</th>\n",
       "      <th>162305</th>\n",
       "      <th>162306</th>\n",
       "      <th>162307</th>\n",
       "      <th>162308</th>\n",
       "      <th>162309</th>\n",
       "      <th>162310</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age_0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 162311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "cid    0       1       2       3       4       5       6       7       8       \\\n",
       "age_0       1       0       1       0       0       0       0       0       0   \n",
       "age_1       0       0       0       0       0       0       1       0       0   \n",
       "age_2       0       0       0       0       0       0       0       0       0   \n",
       "age_3       0       0       0       0       1       0       0       0       1   \n",
       "age_4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "cid    9       ...  162301  162302  162303  162304  162305  162306  162307  \\\n",
       "age_0       0  ...       0       1       0       0       0       0       0   \n",
       "age_1       0  ...       1       0       0       0       0       1       0   \n",
       "age_2       0  ...       0       0       0       0       1       0       0   \n",
       "age_3       0  ...       0       0       1       0       0       0       0   \n",
       "age_4       0  ...       0       0       0       0       0       0       0   \n",
       "\n",
       "cid    162308  162309  162310  \n",
       "age_0       1       0       0  \n",
       "age_1       0       0       0  \n",
       "age_2       0       0       0  \n",
       "age_3       0       1       0  \n",
       "age_4       0       0       0  \n",
       "\n",
       "[5 rows x 162311 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dummy variables for categorical variables\n",
    "shingles = pd.DataFrame(index=customers.index)\n",
    "shingles = pd.concat([shingles, pd.get_dummies(customers['age_group'], prefix='age')], axis=1)\n",
    "shingles = pd.concat([shingles, pd.get_dummies(customers['balance_group'], prefix='balance')], axis=1)\n",
    "shingles = pd.concat([shingles, pd.get_dummies(customers['tamount_group'], prefix='tamount')], axis=1)\n",
    "# transpose to get the shingles\n",
    "shingles = shingles.T\n",
    "shingles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Fingerprint hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sparse matrix for shingles\n",
    "from scipy import sparse\n",
    "\n",
    "def get_signatures(shingles, signature_length: int) -> np.ndarray:\n",
    "    # set seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    signatures = np.full((signature_length, shingles.shape[1]), np.nan, dtype=np.int32)\n",
    "    for i in range(signature_length):\n",
    "\n",
    "        idx = np.random.choice(shingles.index, shingles.shape[0], replace=False)\n",
    "        sparse_shingles = sparse.csc_matrix(shingles.loc[idx].values)\n",
    "        rowidx = sparse_shingles.nonzero()[0]\n",
    "        colidx = sparse_shingles.nonzero()[1]\n",
    "        signatures[i,:] = rowidx[np.unique(colidx, return_index=True)[1]]\n",
    "    return signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = get_signatures(shingles, signature_length=20)\n",
    "signatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_buckets(signatures, band_size):\n",
    "    buckets = {}\n",
    "    rows = signatures.shape[0] // band_size\n",
    "    for i in range(rows):\n",
    "        for j in range(signatures.shape[1]):\n",
    "            bucket = (i, *signatures[i*band_size:(i+1)*band_size, j])\n",
    "            if bucket not in buckets:\n",
    "                buckets[bucket] = []\n",
    "            buckets[bucket].append(j)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if equal\n",
    "band_size = 2\n",
    "buckets = create_buckets(signatures, band_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we first parse query so that it is in the same format as the shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_users = datas.getQueryUsers()\n",
    "query_users['age_group'] = query_users['age'].apply(lambda x: getGroup(age_groups, x))\n",
    "query_users['balance_group'] = query_users['balance'].apply(lambda x: getGroup(balance_groups, x))\n",
    "query_users['tamount_group'] = query_users['tamount'].apply(lambda x: getGroup(tamount_groups, x))\n",
    "query_users.index = range(len(query_users))\n",
    "\n",
    "query_shingles = pd.DataFrame(index=query_users.index)\n",
    "query_shingles = pd.concat([query_shingles, pd.get_dummies(query_users['age_group'], prefix='age')], axis=1)\n",
    "query_shingles = pd.concat([query_shingles, pd.get_dummies(query_users['balance_group'], prefix='balance')], axis=1)\n",
    "query_shingles = pd.concat([query_shingles, pd.get_dummies(query_users['tamount_group'], prefix='tamount')], axis=1)\n",
    "query_shingles = query_shingles.T\n",
    "\n",
    "query_signatures = get_signatures(query_shingles, signature_length=20)\n",
    "\n",
    "query_shingles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented 2 functions to query users. The first takes intersection of all the matching buckets, and the second takes union of all the matching buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(buckets, query_signatures, band_size):\n",
    "\n",
    "    intersection = {j: set() for j in range(query_signatures.shape[1])}\n",
    "    num_bucket_matches = {j: 0 for j in range(query_signatures.shape[1])}\n",
    "\n",
    "    rows = query_signatures.shape[0] // band_size\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(query_signatures.shape[1]):\n",
    "            bucket = (i, *query_signatures[i*band_size:(i+1)*band_size, j])\n",
    "            if bucket in buckets.keys():\n",
    "                intersection[j] = intersection[j] & set(buckets[bucket]) if len(intersection[j]) > 0 else set(buckets[bucket])\n",
    "                num_bucket_matches[j] += 1\n",
    "\n",
    "    return intersection, num_bucket_matches\n",
    "\n",
    "def get_unions(buckets, query_signatures, band_size):\n",
    "\n",
    "    unions = {j: set() for j in range(query_signatures.shape[1])}\n",
    "    num_bucket_matches = {j: 0 for j in range(query_signatures.shape[1])}\n",
    "\n",
    "    rows = query_signatures.shape[0] // band_size\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(query_signatures.shape[1]):\n",
    "            bucket = (i, *query_signatures[i*band_size:(i+1)*band_size, j])\n",
    "            if bucket in buckets.keys():\n",
    "                unions[j] = unions[j].union(buckets[bucket])\n",
    "                num_bucket_matches[j] += 1\n",
    "\n",
    "    return unions, num_bucket_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_size = 2\n",
    "\n",
    "buckets = create_buckets(signatures, band_size=band_size)\n",
    "\n",
    "intersections, _ = get_intersection(buckets, query_signatures, band_size=band_size)\n",
    "print('Number of matching customers by intersection for the first 10 queries:')\n",
    "for i in range(10):\n",
    "    print(len(intersections[i]), end='\\t')\n",
    "\n",
    "\n",
    "print('\\nNumber of matching customers by union for the first 10 queries:')\n",
    "unions, _ = get_unions(buckets, query_signatures, band_size=band_size)\n",
    "for i in range(10):\n",
    "    print(len(unions[i]), end='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, sometimes the intersection of the buckets is empty. In order to consistently rely on this method we must tune the number shingles and bands to make sure that the intersection is not empty. Let's inspect an example where we get a non-empty intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last query user\n",
    "query_users.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.iloc[list(intersections[45])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customers above are pretty similar in their balance, transaction amount, and age. So we can say that our algorithm is working for some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grouping customers together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Set up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Cleaning transactions data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Customer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customersX = datas.getCustomersX().copy()\n",
    "customersX[customersX.columns[:9]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are sparing you to look at all the data cleaning and preprocessing we did, at least in this notebook. You may find the code in datasets.py file. Rather we are going to go over the variables we created and how we created them. So as we have explaiend before we have created new customer IDs. And the data we have for these customers have unique gender, location, balance; thus the 3 of the 7 variables that you asked for is automatically implemented. We have calculated the other 4 variables which were required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customersX[customersX.columns[9:]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, you can see the variables we have added to the data. We have added the following variables:\n",
    "1. **age**: The age of the customer\n",
    "2. **tamount_balance_ratio**: The ratio of the average transaction amount to the balance of the customer\n",
    "3. **log_balance**: The log of the balance of the customer\n",
    "4. **log_tamount**: The log of the average transaction amount of the customer\n",
    "5. **balance_age_ratio**: The ratio of the balance to the age of the customer\n",
    "6. **balance_leq_avg_salary**: Whether the balance of the customer is less than or equal to the average salary in India, that is 3200 INR.\n",
    "7. **top_30_loc**: Whether the customer is from one of the top 30 most frequent locations in the dataset.\n",
    "8. **tranaction_freq**: The frequency of the transactions of the customer.\n",
    "9. **transaction_freq_std**: The standard deviation of the transaction frequency of the customer. This is to indicate how consistent the customer is in terms of transaction frequency.\n",
    "10. **vol_weighted_transaction_freq**: The volume weighted transaction frequency of the customer. This is to indicate how much the customer spends in terms of transaction frequency.\n",
    "11. **t_max**: The maximum transaction amount of the customer.\n",
    "12. **t_min**: The minimum transaction amount of the customer.\n",
    "13. **t_std**: The standard deviation of the transaction amount of the customer.\n",
    "14. **t_skew**: The skewness of the transaction amount of the customer.\n",
    "15. **t_kurt**: The kurtosis of the transaction amount of the customer.\n",
    "16. **tamount_age_ratio**: The ratio of the average transaction amount to the age of the customer.\n",
    "17. **boomer**: Whether the customer is a boomer or not, that is, above 60 years of age.\n",
    "18. **zoomer**: Whether the customer is a zoomer or not, that is, below 20 years of age.\n",
    "19. **days_till_bd**: The number of days till the customer's birthday. This can eliminate extra spending due to customer's birthday.\n",
    "20. **zodiac**: The zodiac sign of the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Choose your features (variables)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will standardize our data before clustering because we want to give **equal importance** to all the variables. We will drop the location, zodiac variables because we believe it will cause overfitting. We will also convert columns with boolean values to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = customersX.copy()\n",
    "X.drop(['dob', 'location', 'zodiac'], axis=1, inplace=True)\n",
    "X.loc[:, X.dtypes == 'bool'] = X.loc[:, X.dtypes == 'bool'].astype('int')\n",
    "X['gender'] = X['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "print('The number of non-numeric features is: ', sum([x not in ['int64', 'float64'] for x in X.dtypes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean()) / X.std()\n",
    "print('The number of cols with absolute mean larger than 0.001 is: ', sum(abs(X.mean()) > 0.001))\n",
    "print('The number of cols with absolute std larger than 1.0001 or smaller than 0.9999 is: ', sum(abs(X.std() - 1) > 0.001) + sum(X.std() < 0.9999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply PCA to reduce the dimensionality of our data. Since we have standardised our data, it is also equivalent to Factor Analysis with the assumption of the noise of the components is isotropic. We also don't use any categorical variables, so we don't need to use Multiple correspondence analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pca\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(X)\n",
    "# Plot the explained variance\n",
    "pd.Series(np.cumsum(pca.explained_variance_ratio_)).plot(\n",
    "    title='Explained variance ratio by PCA components',\n",
    "    figsize=(20, 4),\n",
    "    grid=True,\n",
    "    xlabel='PCA components',\n",
    "    ylabel='Explained variance ratio'\n",
    ")\n",
    "\n",
    "print('The number of PCA components that explain 70% of the variance is: ', np.where(np.cumsum(pca.explained_variance_ratio_) > 0.70)[0][0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=9).fit(X)\n",
    "XPC = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "def assign_cluster(row, centroids):\n",
    "    cluster = np.argmin(np.sum((centroids - row) ** 2, axis=1))\n",
    "    return (cluster, (1, row))\n",
    "\n",
    "\n",
    "def update_centroids(x, y):\n",
    "    # Returns the sum of number of observations and the sum of the rows\n",
    "    return (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "def kmeans(X, k, max_iter=10, epsilon=0.0001, verbose=True):\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    rdd = sc.parallelize(X)\n",
    "\n",
    "    centroids = X[np.random.choice(X.shape[0], k, replace=False), :]\n",
    "    sse = np.inf\n",
    "\n",
    "    for j in (pbar:= tqdm(range(max_iter), desc='KMeans iterations')):\n",
    "        mapResults = rdd.map(lambda x: assign_cluster(x, centroids)).reduceByKey(update_centroids).collect()\n",
    "        centroids = np.array([mapResults[i][1][1] / mapResults[i][1][0] for i in range(k)])\n",
    "\n",
    "        \n",
    "        clusters = [row[0] for row in rdd.map(lambda x: assign_cluster(x, centroids)).collect()]\n",
    "        new_sse = np.sum((np.dot(pd.get_dummies(clusters).values, centroids) - X) ** 2)\n",
    "        \n",
    "        if (new_sse / sse) > (1 - epsilon):\n",
    "            if verbose:\n",
    "                print(f'Converged, in {j + 1} iterations :))')\n",
    "            break\n",
    "        \n",
    "        if verbose:\n",
    "            pbar.set_description(f'KMeans iterations {j + 1}/{max_iter} (SSE: {sse:.2f}), rate: {(new_sse / sse):.5f}')\n",
    "        sse = new_sse\n",
    "\n",
    "\n",
    "    return clusters, centroids, sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters_grid = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "sse_scores = np.inf * np.ones(len(number_of_clusters_grid))\n",
    "num_rep = 1\n",
    "\n",
    "for i, k in enumerate(number_of_clusters_grid):\n",
    "    for j in range(num_rep):\n",
    "        _, _, sse_score = kmeans(XPC, k, 50)\n",
    "        if sse_score < sse_scores[i]:\n",
    "            sse_scores[i] = sse_score\n",
    "\n",
    "pd.Series(sse_scores, index=number_of_clusters_grid).plot(\n",
    "    title='SSE by number of clusters',\n",
    "    figsize=(20, 4),\n",
    "    grid=True,\n",
    "    xlabel='Number of clusters',\n",
    "    ylabel='SSE'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were planning to support our decision with the elbow method, however, as you can see on the plot above, there are no clear \"elbows\". Practically, banks want customer segments that are easy to understand, and reflect real life. So we believe that the number of clusters should be between 3 and 7. Looking at the plot we see decreases in 4 and 6, so we will continue with these two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters4, centroids4, _ = kmeans(XPC, 4, 50)\n",
    "clusters6, centroids6, _ = kmeans(XPC, 6, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot according to the first two principal components\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "# Scaterplot with 4 clusters\n",
    "axs[0].scatter(XPC[:, 0], XPC[:, 1], c=clusters4, cmap='tab10', alpha=0.5)\n",
    "axs[0].scatter(centroids4[:, 0], centroids4[:, 1], marker='x', s=100, c='k')\n",
    "axs[0].set_title('KMeans clustering with 4 clusters')\n",
    "axs[0].set_xlabel('PC1'); axs[0].set_ylabel('PC2')\n",
    "axs[0].set_xlim(-5,20); axs[0].set_ylim(-20,20)\n",
    "axs[0].grid()\n",
    "\n",
    "# Scaterplot with 6 clusters\n",
    "axs[1].scatter(XPC[:, 0], XPC[:, 1], c=clusters6, cmap='tab10', alpha=0.5)\n",
    "axs[1].scatter(centroids6[:, 0], centroids6[:, 1], marker='x', s=100, c='k')\n",
    "axs[1].set_title('KMeans clustering with 6 clusters')\n",
    "axs[1].set_xlabel('PC1'); axs[1].set_ylabel('PC2')\n",
    "axs[1].set_xlim(-5,20); axs[1].set_ylim(-20,20)\n",
    "axs[1].grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Analysing your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select 2-3 variables, that is; gender, age, balance; to create our pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables = ['age', 'gender', 'balance']\n",
    "cust4pivot = customersX.copy()\n",
    "\n",
    "cust4pivot['age_group'] = pd.qcut(customersX['age'], 4, labels=False)\n",
    "cust4pivot['balance_group'] = pd.qcut(customersX['balance'], 4, labels=False)\n",
    "\n",
    "# make dummies\n",
    "cust4pivot = pd.get_dummies(cust4pivot, columns=['gender', 'age_group', 'balance_group'])\n",
    "\n",
    "# pivot table\n",
    "# gender_0\tgender_1\tage_group_0\tage_group_1\tage_group_2\tage_group_3\tbalance_group_0\tbalance_group_1\tbalance_group_2\tbalance_group_3\n",
    "cols = ['gender_0', 'gender_1', 'age_group_0', 'age_group_1', 'age_group_2', 'age_group_3', 'balance_group_0', 'balance_group_1', 'balance_group_2', 'balance_group_3']\n",
    "cust4pivot = cust4pivot.pivot_table(index='clusters4', values=cols, aggfunc='mean')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "cust4pivot.columns = ['below 25', '25 to 28', '28 to 34', '34+', 'below 5240 INR', '5240 to 17230 INR', '17230 to 55530 INR', '55530+ INR', 'F', 'M']\n",
    "cust4pivot = 100 * cust4pivot\n",
    "cust4pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our indepth analysis of the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_inspect = ['age', 'gender', 'balance', 'log_balance', 'avg_tamount', 'top_30_loc', 'transaction_freq']\n",
    "variables_to_inspect = [*customersX.columns[1:2], *customersX.columns[4:25]]\n",
    "customersX['clusters4'] = clusters4\n",
    "customersX['clusters6'] = clusters6\n",
    "customersX['gender'] = customersX['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "c4stomers = customersX.groupby('clusters4').agg({col: 'mean' for col in variables_to_inspect})\n",
    "(c4stomers - customersX[variables_to_inspect].mean()) / customersX[variables_to_inspect].std()\n",
    "# merge with columns\n",
    "Z_scores = (c4stomers - customersX[variables_to_inspect].mean()) / customersX[variables_to_inspect].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "# Bar plot\n",
    "def colors_from_values(values, palette_name):\n",
    "    # normalize the values to range [0, 1]\n",
    "    normalized = values\n",
    "    normalized[normalized < 0] = - normalized[normalized < 0] / normalized.min() * 500\n",
    "    normalized[normalized > 0] = normalized[normalized > 0] / normalized.max() * 500 -1\n",
    "    normalized = normalized + 500\n",
    "    # convert to indices\n",
    "    indices = np.round(normalized).astype(np.int32)\n",
    "    # use the indices to get the colors\n",
    "    palette = sns.color_palette(palette_name, 1000)\n",
    "    return np.array(palette).take(indices, axis=0)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "fig.suptitle('Z-scores for each cluster', fontsize=16)\n",
    "\n",
    "# 1st cluster bar plot\n",
    "sns.barplot(x=Z_scores.iloc[0, :].sort_values(ascending=False), y=Z_scores.iloc[0, :].sort_values(ascending=False).index,\n",
    "ax=axs[0, 0], palette=colors_from_values(Z_scores.iloc[0, :].sort_values(ascending=False), 'Spectral'), orient = 'h')\n",
    "axs[0, 0].set_title('Irregular spenders')\n",
    "\n",
    "# 2nd cluster bar plot\n",
    "sns.barplot(x=Z_scores.iloc[1, :].sort_values(ascending=False), y=Z_scores.iloc[1, :].sort_values(ascending=False).index,\n",
    "ax=axs[0, 1], palette=colors_from_values(Z_scores.iloc[1, :].sort_values(ascending=False), 'Spectral'), orient = 'h')\n",
    "axs[0, 1].set_title('Poor folk')\n",
    "\n",
    "# 3rd cluster bar plot\n",
    "sns.barplot(x=Z_scores.iloc[2, :].sort_values(ascending=False), y=Z_scores.iloc[2, :].sort_values(ascending=False).index,\n",
    "ax=axs[1, 0], palette=colors_from_values(Z_scores.iloc[2, :].sort_values(ascending=False), 'Spectral'), orient = 'h')\n",
    "axs[1, 0].set_title('Conservatives')\n",
    "\n",
    "# 4th cluster bar plot\n",
    "sns.barplot(x=Z_scores.iloc[3, :].sort_values(ascending=False), y=Z_scores.iloc[3, :].sort_values(ascending=False).index,\n",
    "ax=axs[1, 1], palette=colors_from_values(Z_scores.iloc[3, :].sort_values(ascending=False), 'Spectral'), orient = 'h')\n",
    "axs[1, 1].set_title('Rich folk')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pivot table we get a general idea of clusters. However, we thought it did not unravel the complete story. So we decided to look at the clusters features in terms of their distance from to overall means. In the plot above, you can see the distance of each cluster from the overall mean. The features that have distances with high magnitude characterize their clusters. So here is our interpretation of the clusters:\n",
    "\n",
    "**Cluster 1**: This cluster is characterized by the fact that the customers have dispersed spending habits, because their kurtosis and skewness for their transaction amounts are high compared to the rest of dataset. Their minimum transactions are very low and maximum are very high, which means they use their cards for any kind of spending. As a bank we would like identify these customers of being more adapted to the digital world, and offer them more digital services.\n",
    "\n",
    "**Cluster 2**: This cluster is the poor people. They have balances lower than the average salary. This means that they are more likely default on their payments. As a bank we would be more careful with these customers, and offer them more services that would help them manage their finances better.\n",
    "\n",
    "**Cluster 3**: We can label this cluster as the \"conservatives\" or \"traditionalists\". They have low number of transactions, and relatively old. They only use their cards to pay very high amounts. This means that they do not use their cards for small purchases. As a bank we would target them as customers with more potential to spend, and try to educate them on the benefits of using their cards for small purchases.\n",
    "\n",
    "**Cluster 4**: Eat the rich. This cluster is characterized by the fact that they have very high balances, and very high transaction amounts. They are also very old and boomer. We would offer them personalized services, and try to make them feel special. For example, lounge access, and private transfers etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10).fit(XPC[:, :2])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(XPC[:, 0], XPC[:, 1], c=dbscan.labels_, cmap='tab10', alpha=0.5)\n",
    "plt.title('DBSCAN clustering')\n",
    "\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.xlim(-5,20); plt.ylim(-20,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is very dense, in the center and sparse in the edges. This is because, economically, the majority of the population is in the low-middle class, and the upper class is very small. As a result, DBSCAN groups all low and middle class customers together, and leaves the upper class customers alone. The distance between the middle and lower class is very small compared to the distance between the upper class and the middle class. Therefore density is not a good measure and **DBSCAN** is not a good clustering method for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical = AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    linkage='ward',\n",
    "    distance_threshold = 7\n",
    ").fit(XPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dendrogram\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count +=1\n",
    "            else:\n",
    "                current_count += counts[child_idx-n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                     counts]).astype(float)\n",
    "    \n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "ax.set_title(\"Hierarchical clustering dendrogram\")\n",
    "#plot the top three levels of the dendrogram\n",
    "plot_dendrogram(hierarchical, truncate_mode='level', p=7)\n",
    "ax.set_xlabel(\"Number of points in node\")\n",
    "plt.show()\n",
    "\n",
    "# This code is inspired from Philip Wilkinson's post on Towards Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical clustering with 4 clusters\n",
    "hierarchical4 = AgglomerativeClustering(\n",
    "    n_clusters=4,\n",
    "    linkage='ward',\n",
    "    distance_threshold = None\n",
    ").fit(XPC)\n",
    "\n",
    "# get the cluster labels\n",
    "hierarchical4.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customersX['hierarchical4'] = hierarchical4.labels_\n",
    "customers_h4 = customersX.groupby('hierarchical4').agg({col: 'mean' for col in variables_to_inspect})\n",
    "Z_h4 = (customers_h4 - customersX[variables_to_inspect].mean()) / customersX[variables_to_inspect].std()\n",
    "\n",
    "for i in range(4):\n",
    "    print(f'Top explanatory features for Cluster {i+1}')\n",
    "    t = Z_h4.loc[i, Z_h4.iloc[i, :].abs().sort_values(ascending=False)[:5].index].sort_values(ascending=False)\n",
    "    for j in range(len(t)):\n",
    "        print(f'{j+1}. {t.index[j]}: \\t \\t {t.iloc[j].round(2)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hierarchical clustering, we have decided to inspect 4 clusters. One immediate observation is that the some clusters are way more marginal compared to the results we got in K-means. As you can see, cluster 1 and 4 has features that are very distant from the mean, which is a sign of being even more charactestic. For example, cluster 4 are people with very high balance and age. In our previous results we have also identified a segment of rich customers, however, the results we get from hierarchical clustering are more extreme. We can further our analysis looking into more clusters, that is, dividing up the current clusters into more clusters. And doing so we can better understand the segments of customers. We think this is a good method to use for a bank to understand their customers, where computational power is not a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kmeans from sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans_sk = KMeans(n_clusters=n_clusters)\n",
    "rows = 5\n",
    "fig, axs = plt.subplots(rows, 2, figsize=(20, 25))\n",
    "\n",
    "for i in range(rows):\n",
    "    kmeans_sk.fit(XPC[:, :2])\n",
    "    axs[i, 0].scatter(XPC[:, 0], XPC[:, 1], c=kmeans_sk.labels_, cmap='tab10', alpha=0.5)\n",
    "    axs[i, 0].set_title(f'K-means clustering (sklearn) {i+1}')\n",
    "    axs[i, 0].set_xlabel('PC1'); axs[i, 0].set_ylabel('PC2')\n",
    "    axs[i, 0].set_xlim(-5,20); axs[i, 0].set_ylim(-20,20)\n",
    "\n",
    "    clusters, centroids, _ = kmeans(XPC[:, :2], k=n_clusters, max_iter=100, verbose=False)\n",
    "    axs[i, 1].scatter(XPC[:, 0], XPC[:, 1], c=clusters, cmap='tab10', alpha=0.5)\n",
    "    axs[i, 1].set_title(f'K-means clustering (our code) {i+1}')\n",
    "    axs[i, 1].set_xlabel('PC1'); axs[i, 1].set_ylabel('PC2')\n",
    "    axs[i, 1].set_xlim(-5,20); axs[i, 1].set_ylim(-20,20)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code we initialize randomly and the module we have imported from sklearn initialize with KMeans++. In the plot above, you can observe that the module we have imported from sklearn always has only 2 large clusters, and the third is not even visible. On the other hand, in some of the results that we get from our code has 3 balanced clusters. This is due to the nature of clustering processes. Clustering, in the end, has qualitative implications, and sometimes quantitative metrics do not give a good picture of the results. Surprisingly, our code does a better job in having more balanced clusters, if that is what you are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of clustering comparison\n",
    "\n",
    "All in all, we have learned that not every clustering algorithm fits every dataset. In our case, DBSCAN is not a good choice because of the distribution of our data, compared to K-means and hierarchical clustering. Moreover, the motivation behind the clustering is also a factor that should be considered. For example, if you want to have a balanced number of clusters, in our case, K-means with random initiliziation produced better results. Finally, we think that if our goal is to do qualitative analysis, then we should use hierarchical clustering, since it provides sub-clusters, and we can further our analysis by looking into the sub-clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo '1-The location that has the maximum number of purchases been made:'\n",
    "\n",
    "awk -F, '{print($5)}' data/bank_transactions.csv | sort | uniq -c | sort -nr | head -1\n",
    "\n",
    "echo '2-The most spending purchases are: '\n",
    "\n",
    "awk -F, '{$4+=$9}END{ if(list[\"F\"] > list[\"M\"]){ print(\"F\");} else{ print(\"M\");}}' data/bank_transactions.csv\n",
    "\n",
    "echo '3-The customer with the highest average transaction amount: '\n",
    "\n",
    "awk -F, 'BEGIN{FS=\",\"} {if(NR>1){arr[$2]= arr[$2]+$9; count[$2]++}} END{for(c in arr){arr[c]= arr[c]/count[c]} asort(arr, sortedarr);for(l in arr){if(arr[l]==sortedarr[length(sortedarr)])print arr[l], l}}' bank_transactions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_enough_guards(N: int, M: int, G: int, arriving: list):\n",
    "    \"\"\"\n",
    "    Check if there are enough guards to protect the gates\n",
    "    \"\"\"\n",
    "    enough  = True # do we have enough guards?\n",
    "    history = {}   # history of first and last time a student visited a gate\n",
    "\n",
    "    for i, student in enumerate(arriving):\n",
    "\n",
    "        if student not in history.keys():\n",
    "            history[student] = [i, -1] # first time, last time\n",
    "        else:\n",
    "            history[student][1] = i    # update last time\n",
    "        \n",
    "\n",
    "    for i in range(N):\n",
    "        intersections = 0\n",
    "\n",
    "        for gate in history.keys():\n",
    "            if history[gate][0] <= i <= history[gate][1]:\n",
    "                intersections += 1\n",
    "        \n",
    "        if intersections > G:\n",
    "            enough = False\n",
    "            break\n",
    "\n",
    "    print('YES') if enough else print('NO')\n",
    "    # print(history, interactions)\n",
    "\n",
    "## Case 1\n",
    "arriving = [1, 1, 3, 3, 3]\n",
    "#arriving = [3, 3, 1, 1, 3, 3, 2, 2, 3, 3, 2, 2,]\n",
    "\n",
    "N, M, G = 4, len(arriving), 1\n",
    "\n",
    "print(f'N = {N}, M = {M}, G = {G}, arriving = {arriving}')\n",
    "check_enough_guards(N, M, G, arriving)\n",
    "\n",
    "\n",
    "# Case 2\n",
    "arriving = [1, 2, 1, 2, 2]\n",
    "N, M, G = 2, len(arriving), 1\n",
    "\n",
    "print(f'N = {N}, M = {M}, G = {G}, arriving = {arriving}')\n",
    "check_enough_guards(N, M, G, arriving)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2875e863656f51b086e8e28d9d22dbee1e08a4c40db222f2cb26ce4a6f7eef6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
